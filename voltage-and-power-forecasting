#!/usr/bin/env python
# -*- coding:utf-8 -*-
import torch
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.nn.functional as F
from math import sqrt
from torch import optim
import time
import warnings
from torch.utils.data import Dataset, DataLoader, TensorDataset
import math
from pathlib import Path

warnings.filterwarnings('ignore')


class Dataset_generation(Dataset):
    def __init__(self, path_train_enc, path_train_dec, path_train_output,
                 path_vali_enc, path_vali_dec, path_vali_output, batch_size, seq_len, label_len, pred_len):
        self.path_train_enc = path_train_enc
        self.path_train_dec = path_train_dec
        self.path_train_output = path_train_output
        self.path_vali_enc = path_vali_enc
        self.path_vali_dec = path_vali_dec
        self.path_vali_output = path_vali_output
        self.batch_size = batch_size
        self.seq_len = seq_len
        self.label_len = label_len
        self.pred_len = pred_len
        self.train_enc = None
        self.train_dec = None
        self.train_output = None
        self.vali_enc = None
        self.vali_dec = None
        self.vali_output = None
        self.train_i0_mean = None
        self.train_time_mean = None
        self.train_i0_std = None
        self.train_time_std = None
        self.train_enc_mark = None
        self.train_dec_mark = None
        self.vali_enc_mark = None
        self.vali_dec_mark = None

    def load_dataframe(self):
        self.train_enc = np.array(pd.read_csv(self.path_train_enc, header=None))
        self.train_dec = np.array(pd.read_csv(self.path_train_dec, header=None))
        self.train_output = np.array(pd.read_csv(self.path_train_output, header=None))
        self.vali_enc = np.array(pd.read_csv(self.path_vali_enc, header=None))
        self.vali_dec = np.array(pd.read_csv(self.path_vali_dec, header=None))
        self.vali_output = np.array(pd.read_csv(self.path_vali_output, header=None))

    def mask_decoder(self, dec):

        dec[:, self.label_len:, :] = 0
        return dec

    def norm(self):
        self.train_i0_mean = np.mean(self.train_enc[:, 1::3].reshape(-1, 1), axis=0)
        self.train_time_mean = np.mean(self.train_enc[:, 2::3].reshape(-1, 1), axis=0)
        self.train_i0_std = np.std(self.train_enc[:, 1::3].reshape(-1, 1), axis=0)
        self.train_time_std = np.std(self.train_enc[:, 2::3].reshape(-1, 1), axis=0)

        self.train_enc[:, ::3] = (self.train_enc[:, ::3] - 2.7) / (4.2 - 2.7)
        self.train_enc[:, 1::3] = (self.train_enc[:, 1::3] - self.train_i0_mean) / self.train_i0_std
        self.train_enc[:, 2::3] = (self.train_enc[:, 2::3] - self.train_time_mean) / self.train_time_std

        self.vali_enc[:, ::3] = (self.vali_enc[:, ::3] - 2.7) / (4.2 - 2.7)
        self.vali_enc[:, 1::3] = (self.vali_enc[:, 1::3] - self.train_i0_mean) / self.train_i0_std
        self.vali_enc[:, 2::3] = (self.vali_enc[:, 2::3] - self.train_time_mean) / self.train_time_std

        self.train_dec[:, ::3] = (self.train_dec[:, ::3] - 2.7) / (4.2 - 2.7)
        self.train_dec[:, 1::3] = (self.train_dec[:, 1::3] - self.train_i0_mean) / self.train_i0_std
        self.train_dec[:, 2::3] = (self.train_dec[:, 2::3] - self.train_time_mean) / self.train_time_std

        self.vali_dec[:, ::3] = (self.vali_dec[:, ::3] - 2.7) / (4.2 - 2.7)
        self.vali_dec[:, 1::3] = (self.vali_dec[:, 1::3] - self.train_i0_mean) / self.train_i0_std
        self.vali_dec[:, 2::3] = (self.vali_dec[:, 2::3] - self.train_time_mean) / self.train_time_std

        self.train_output[:, ::3] = (self.train_output[:, ::3] - 2.7) / (4.2 - 2.7)
        self.train_output[:, 1::3] = (self.train_output[:, 1::3] - self.train_i0_mean) / self.train_i0_std
        self.train_output[:, 2::3] = (self.train_output[:, 2::3] - self.train_time_mean) / self.train_time_std

        self.vali_output[:, ::3] = (self.vali_output[:, ::3] - 2.7) / (4.2 - 2.7)
        self.vali_output[:, 1::3] = (self.vali_output[:, 1::3] - self.train_i0_mean) / self.train_i0_std
        self.vali_output[:, 2::3] = (self.vali_output[:, 2::3] - self.train_time_mean) / self.train_time_std

    def split_mark(self, data, state='predict_voltage'):
        if state == 'predict_voltage':
            data_enc = data[:, :, 0][:, :, np.newaxis]
            data_mark = data[:, :, 1:]
        else:
            data_enc = data[:, :, :-1]
            data_mark = data[:, :, -1][:, :, np.newaxis]
        return data_enc, data_mark

    def return_dataloader(self, state='predict_voltage'):
        self.load_dataframe()
        self.norm()
        self.train_enc = self.train_enc.reshape(-1, self.seq_len, 3)
        self.train_dec = self.train_dec.reshape(-1, self.label_len + self.pred_len, 3)
        self.train_output = self.train_output.reshape(-1, self.pred_len, 3)
        self.vali_enc = self.vali_enc.reshape(-1, self.seq_len, 3)
        self.vali_dec = self.vali_dec.reshape(-1, self.label_len + self.pred_len, 3)
        self.vali_output = self.vali_output.reshape(-1, self.pred_len, 3)
        self.train_enc, self.train_enc_mark = self.split_mark(self.train_enc, state)
        self.train_dec, self.train_dec_mark = self.split_mark(self.train_dec, state)
        self.vali_enc, self.vali_enc_mark = self.split_mark(self.vali_enc, state)
        self.vali_dec, self.vali_dec_mark = self.split_mark(self.vali_dec, state)
        self.train_dec = self.mask_decoder(self.train_dec)
        self.vali_dec = self.mask_decoder(self.vali_dec)
        if state == 'predict_voltage':
            self.train_output = self.train_output[:, :, 0][:, :, np.newaxis]
            self.vali_output = self.vali_output[:, :, 0][:, :, np.newaxis]
        else:
            self.train_output = self.train_output[:, :, :-1]
            self.vali_output = self.vali_output[:, :, :-1]
        print(self.train_enc.shape, self.train_enc_mark.shape, self.train_dec.shape, self.train_dec_mark.shape
              , self.train_output.shape,
              self.vali_enc.shape, self.vali_enc_mark.shape, self.vali_dec.shape, self.vali_dec_mark.shape
              , self.vali_output.shape)
        print(self.train_enc[:16,:,:].shape, self.train_enc_mark[:16,:,:].shape, self.train_dec[:16,:,:].shape,
              self.train_dec_mark[:16,:,:].shape, self.train_output[:16,:,:].shape)
        train_set = TensorDataset(torch.from_numpy(self.train_enc).to(torch.float32),
                                  torch.from_numpy(self.train_enc_mark).to(torch.float32),
                                  torch.from_numpy(self.train_dec).to(torch.float32),
                                  torch.from_numpy(self.train_dec_mark).to(torch.float32),
                                  torch.from_numpy(self.train_output).to(torch.float32))
        vali_set = TensorDataset(torch.from_numpy(self.vali_enc).to(torch.float32),
                                 torch.from_numpy(self.vali_enc_mark).to(torch.float32),
                                 torch.from_numpy(self.vali_dec).to(torch.float32),
                                 torch.from_numpy(self.vali_dec_mark).to(torch.float32),
                                 torch.from_numpy(self.vali_output).to(torch.float32))
        test_set = TensorDataset(torch.from_numpy(self.train_enc[:16,:,:]).to(torch.float32),
                                  torch.from_numpy(self.train_enc_mark[:16,:,:]).to(torch.float32),
                                  torch.from_numpy(self.train_dec[:16,:,:]).to(torch.float32),
                                  torch.from_numpy(self.train_dec_mark[:16,:,:]).to(torch.float32),
                                  torch.from_numpy(self.train_output[:16,:,:]).to(torch.float32))
        train_loader = DataLoader(train_set, batch_size=self.batch_size)
        vali_loader = DataLoader(vali_set, batch_size=self.batch_size)
        test_loader = DataLoader(test_set, batch_size=self.batch_size)
        return train_loader, vali_loader, test_loader

def RSE(pred, true):
    return np.sqrt(np.sum((true - pred) ** 2)) / np.sqrt(np.sum((true - true.mean()) ** 2))


def CORR(pred, true):
    u = ((true - true.mean(0)) * (pred - pred.mean(0))).sum(0)
    d = np.sqrt(((true - true.mean(0)) ** 2 * (pred - pred.mean(0)) ** 2).sum(0))
    return (u / d).mean(-1)


def MAE(pred, true):
    return np.mean(np.abs(pred - true))


def MSE(pred, true):
    return np.mean((pred - true) ** 2)


def RMSE(pred, true):
    return np.sqrt(MSE(pred, true))


def MAPE(pred, true):
    return np.mean(np.abs((pred - true) / true))


def MSPE(pred, true):
    return np.mean(np.square((pred - true) / true))


def metric(pred, true):
    mae = MAE(pred, true)
    mse = MSE(pred, true)
    rmse = RMSE(pred, true)
    mape = MAPE(pred, true)
    mspe = MSPE(pred, true)

    return mae, mse, rmse, mape, mspe


def adjust_learning_rate(optimizer, epoch, learning_rate, lradj):
    if lradj == 'type1':
        lr_adjust = {epoch: learning_rate * (0.8 ** ((epoch - 1) // 2))}
    elif lradj == 'type2':
        lr_adjust = {
            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,
            10: 5e-7, 15: 1e-7, 20: 5e-8
        }
    if epoch in lr_adjust.keys():
        lr = lr_adjust[epoch]
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        print('Updating learning rate to {}'.format(lr))


class EarlyStopping:
    def __init__(self, patience=7, verbose=False, delta=0):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf
        self.delta = delta

    def __call__(self, val_loss, model, path):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path)
        elif score < self.best_score + self.delta:
            self.counter += 1
            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path)
            self.counter = 0

    def save_checkpoint(self, val_loss, model, path):
        if self.verbose:
            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')
        torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')
        self.val_loss_min = val_loss


class TokenEmbedding_for_mark(nn.Module):
    def __init__(self, c_in, d_model): 
        super(TokenEmbedding_for_mark, self).__init__()
        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,
                                   kernel_size=3, padding=1, padding_mode='circular')
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')

    def forward(self, x):
        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
        return x


class TokenEmbedding(nn.Module):
    def __init__(self, c_in, d_model):  
        super(TokenEmbedding, self).__init__()
        padding = 1 
        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,
                                   kernel_size=3, padding=1, padding_mode='circular')
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')

    def forward(self, x):
        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
        return x


class PositionalEmbedding(nn.Module):
    def __init__(self, d_model, max_len=2000):
        super(PositionalEmbedding, self).__init__()
        pe = torch.zeros(max_len, d_model).float()  
        pe.require_grad = False

        position = torch.arange(0, max_len).float().unsqueeze(1)  
        div_term = (torch.arange(0, d_model, 2).float() * -(
                math.log(10000.0) / d_model)).exp() 

        pe[:, 0::2] = torch.sin(position * div_term)  
        pe[:, 1::2] = torch.cos(position * div_term)  

        pe = pe.unsqueeze(0)  
        self.register_buffer('pe', pe)  

    def forward(self, x):
        return self.pe[:, :x.size(1)] 


class DataEmbedding(nn.Module):
    def __init__(self, c_in, d_model, dropout=0.05):
        super(DataEmbedding, self).__init__()

        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)  
        self.position_embedding = PositionalEmbedding(d_model=d_model) 
        self.mark_embedding = TokenEmbedding_for_mark(c_in=3 - c_in, d_model=d_model)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x, x_mark):
        x = self.value_embedding(x) + self.position_embedding(x) + self.mark_embedding(x_mark)

        return self.dropout(x)


class FullAttention(nn.Module):
    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):
        super(FullAttention, self).__init__()
        self.scale = scale
        self.mask_flag = mask_flag 
        self.output_attention = output_attention
        self.dropout = nn.Dropout(attention_dropout)

    def forward(self, queries, keys, values, attn_mask):
        B, L, H, E = queries.shape
        _, S, _, D = values.shape
        scale = self.scale or 1. / sqrt(E)

        scores = torch.einsum("blhe,bshe->bhls", queries, keys)
        if self.mask_flag:
            if attn_mask is None:
                attn_mask = TriangularCausalMask(B, L, device=queries.device)

            scores.masked_fill_(attn_mask.mask, -np.inf)

        A = self.dropout(torch.softmax(scale * scores, dim=-1))
        V = torch.einsum("bhls,bshd->blhd", A, values)

        if self.output_attention:
            return (V.contiguous(), A)
        else:
            return (V.contiguous(), None)


class AttentionLayer(nn.Module):
    def __init__(self, attention, d_model, n_heads,
                 d_keys=None, d_values=None, mix=False):
        super(AttentionLayer, self).__init__()

        d_keys = d_keys or (d_model // n_heads) 
        d_values = d_values or (d_model // n_heads)  

        self.inner_attention = attention  
        self.query_projection = nn.Linear(d_model, d_keys * n_heads)  
        self.key_projection = nn.Linear(d_model, d_keys * n_heads)
        self.value_projection = nn.Linear(d_model, d_values * n_heads)
        self.out_projection = nn.Linear(d_values * n_heads, d_model)
        self.n_heads = n_heads
        self.mix = mix  

    def forward(self, queries, keys, values, attn_mask):
        B, L, _ = queries.shape
        _, S, _ = keys.shape
        H = self.n_heads  
        queries = self.query_projection(queries).view(B, L, H, -1)  
        keys = self.key_projection(keys).view(B, S, H, -1) 
        values = self.value_projection(values).view(B, S, H, -1)  

        out, attn = self.inner_attention(
            queries,
            keys,
            values,
            attn_mask  
        ) 
        if self.mix: 
            out = out.transpose(2, 1).contiguous()  
        out = out.view(B, L, -1) 

        return self.out_projection(out), attn 


class DecoderLayer(nn.Module):

    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,
                 dropout=0.1, activation="relu"):
        super(DecoderLayer, self).__init__()
        d_ff = d_ff or 4 * d_model
        self.self_attention = self_attention  
        self.cross_attention = cross_attention  
        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)
        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = F.relu if activation == "relu" else F.gelu 

    def forward(self, x, cross, x_mask=None, cross_mask=None):
        x = x + self.dropout(self.self_attention(
            x, x, x,
            attn_mask=x_mask
        )[0])
        x = self.norm1(x)

        x = x + self.dropout(self.cross_attention(
            x, cross, cross,
            attn_mask=cross_mask
        )[0])

        y = x = self.norm2(x)
        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))
        y = self.dropout(self.conv2(y).transpose(-1, 1))

        return self.norm3(x + y)


class Decoder(nn.Module):
    def __init__(self, layers, norm_layer=None):
        super(Decoder, self).__init__()
        self.layers = nn.ModuleList(layers)
        self.norm = norm_layer

    def forward(self, x, cross, x_mask=None, cross_mask=None): 
        for layer in self.layers:
            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)

        if self.norm is not None:
            x = self.norm(x)

        return x


class ConvLayer(nn.Module):
    def __init__(self, c_in):
        super(ConvLayer, self).__init__()
        padding = 1 if torch.__version__ >= '1.5.0' else 2
        self.downConv = nn.Conv1d(in_channels=c_in,
                                  out_channels=c_in,
                                  kernel_size=3,
                                  padding=padding,
                                  padding_mode='circular')
        self.norm = nn.BatchNorm1d(c_in)
        self.activation = nn.ELU()
        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)

    def forward(self, x):
        x = self.downConv(x.permute(0, 2, 1))
        x = self.norm(x)
        x = self.activation(x)
        x = self.maxPool(x)
        x = x.transpose(1, 2)
        return x


class EncoderLayer(nn.Module):
    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation="relu"):
        super(EncoderLayer, self).__init__()
        d_ff = d_ff or 4 * d_model
        self.attention = attention  
        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)
        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = F.relu if activation == "relu" else F.gelu

    def forward(self, x, attn_mask=None):
        
        new_x, attn = self.attention(
            x, x, x,
            attn_mask=attn_mask
        )  
        x = x + self.dropout(new_x) 

        y = x = self.norm1(x) 
        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))
        y = self.dropout(self.conv2(y).transpose(-1, 1)) 

        return self.norm2(x + y), attn


class Encoder(nn.Module):
    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):
        super(Encoder, self).__init__()
        self.attn_layers = nn.ModuleList(attn_layers) 
        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None
        self.norm = norm_layer

    def forward(self, x, attn_mask=None):
        # x [B, L, D]
        attns = []
        if self.conv_layers is not None:
            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers): 
                x, attn = attn_layer(x, attn_mask=attn_mask)  
                x = conv_layer(x) 
                attns.append(attn)
            x, attn = self.attn_layers[-1](x, attn_mask=attn_mask)
            attns.append(attn)
        else:
            for attn_layer in self.attn_layers:
                x, attn = attn_layer(x, attn_mask=attn_mask)
                attns.append(attn)

        if self.norm is not None:
            x = self.norm(x)
        return x, attns


class EncoderStack(nn.Module):
    def __init__(self, encoders, inp_lens):
        super(EncoderStack, self).__init__()
        self.encoders = nn.ModuleList(encoders)
        self.inp_lens = inp_lens

    def forward(self, x, attn_mask=None):
        x_stack = []
        attns = []
        for i_len, encoder in zip(self.inp_lens, self.encoders):  # 3ä¸ª
            inp_len = x.shape[1] // (2 ** i_len)
            x_s, attn = encoder(x[:, -inp_len:, :])
            x_stack.append(x_s)
            attns.append(attn)
        x_stack = torch.cat(x_stack, -2)

        return x_stack, attns



class Stack(nn.Module):
    def __init__(self, enc_in, dec_in, c_out, seq_len, label_len, out_len,
                 factor=5, d_model=512, n_heads=8, e_layers=[3, 2, 1], d_layers=2, d_ff=2048,
                 dropout=0.05, attn='full', activation='gelu',
                 output_attention=False, distil=True, mix=True,
                 device=torch.device('cuda:0')):
        super(Stack, self).__init__()
        self.pred_len = out_len
        self.attn = attn
        self.output_attention = output_attention

        self.enc_embedding = DataEmbedding(enc_in, d_model, dropout)
        self.dec_embedding = DataEmbedding(dec_in, d_model, dropout)
        Attn = FullAttention
        inp_lens = list(range(len(e_layers)))  # [0,1,2,...] you can customize here
        encoders = [
            Encoder(
                [
                    EncoderLayer(
                        AttentionLayer(
                            Attn(False, factor, attention_dropout=dropout, output_attention=output_attention),
                            d_model, n_heads, mix=False),
                        d_model,
                        d_ff,
                        dropout=dropout,
                        activation=activation
                    ) for l in range(el)
                ],
                [
                    ConvLayer(
                        d_model
                    ) for l in range(el - 1)
                ] if distil else None,
                norm_layer=torch.nn.LayerNorm(d_model)
            ) for el in e_layers]
        self.encoder = EncoderStack(encoders, inp_lens)
        self.decoder = Decoder(
            [
                DecoderLayer(
                    AttentionLayer(Attn(True, factor, attention_dropout=dropout, output_attention=False),
                                   d_model, n_heads, mix=mix),
                    AttentionLayer(FullAttention(False, factor, attention_dropout=dropout, output_attention=False),
                                   d_model, n_heads, mix=False),
                    d_model,
                    d_ff,
                    dropout=dropout,
                    activation=activation,
                )
                for l in range(d_layers)
            ],
            norm_layer=torch.nn.LayerNorm(d_model)
        )
        self.projection = nn.Linear(d_model, c_out, bias=True)

    def forward(self, x_enc, x_enc_mark, x_dec, x_dec_mark,
                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):
        enc_out = self.enc_embedding(x_enc, x_enc_mark)
        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)

        dec_out = self.dec_embedding(x_dec, x_dec_mark)
        dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)
        dec_out = self.projection(dec_out)
        if self.output_attention:
            return dec_out[:, -self.pred_len:, :], attns
        else:
            return dec_out[:, -self.pred_len:, :] 


class Exp():
    def __init__(self, enc_in, dec_in, c_out, seq_len, label_len, out_len, factor, d_model, n_heads, e_layers,
                 d_layers, d_ff, dropout, attn, activation, output_attention, distil, mix, patience):
        super(Exp, self).__init__()
        self.enc_in = enc_in
        self.dec_in = dec_in
        self.c_out = c_out
        self.seq_len = seq_len
        self.label_len = label_len
        self.out_len = out_len
        self.factor = factor
        self.d_model = d_model
        self.n_heads = n_heads
        self.e_layers = e_layers
        self.d_layers = d_layers
        self.d_ff = d_ff
        self.dropout = dropout
        self.attn = attn
        self.activation = activation
        self.output_attention = output_attention
        self.distil = distil
        self.mix = mix
        self.patience = patience
        self.learning_rate = 0.001
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = Stack(enc_in, dec_in, c_out, seq_len, label_len, out_len,
                                   factor, d_model, n_heads, e_layers, d_layers, d_ff,
                                   dropout, attn, activation,
                                   output_attention, distil, mix).to(self.device)
        self.checkpoints_path = './checkpoint/900'

    def _select_optimizer(self):
        model_optim = optim.Adam(self.model.parameters(), lr=self.learning_rate)
        return model_optim

    def _select_criterion(self):
        criterion = nn.MSELoss()
        return criterion

    def vali(self, vali_loader, criterion):
        self.model.eval()  
        total_loss = []
        for i, (batch_enc, batch_enc_mark, batch_dec, batch_dec_mark, batch_output) in enumerate(vali_loader):
            batch_prediction, batch_output = self._process_one_batch(batch_enc, batch_enc_mark,
                                                                     batch_dec, batch_dec_mark, batch_output)
            loss = criterion(batch_prediction.detach().cpu(), batch_output.detach().cpu())
            total_loss.append(loss)
        total_loss = np.average(total_loss)
        self.model.train()
        return total_loss

    def train(self, train_loader, vali_loader, test_loader, train_epochs):
        my_file = Path(self.checkpoints_path + '/' + 'checkpoint.pth')
        if my_file.exists():
            best_model_path = self.checkpoints_path + '/' + 'checkpoint.pth'
            self.model.load_state_dict(torch.load(best_model_path))
            print('have loader best model')
        time_now = time.time()
        train_steps = len(train_loader)
        early_stopping = EarlyStopping(self.patience, verbose=True)
        model_optim = self._select_optimizer()  
        criterion = self._select_criterion() 
        for epoch in range(train_epochs):
            iter_count = 0
            train_loss = []
            epoch_time = time.time()
            self.model.train()
            for i, (batch_enc, batch_enc_mark, batch_dec, batch_dec_mark, batch_output) in enumerate(train_loader):
                iter_count += 1
                model_optim.zero_grad()
                batch_prediction, batch_output = self._process_one_batch(batch_enc, batch_enc_mark,
                                                                         batch_dec, batch_dec_mark, batch_output)
                loss = criterion(batch_prediction, batch_output)
                train_loss.append(loss.item())
                if (i + 1) % 100 == 0:
                    print("\titers: {0}, epoch: {1} | loss: {2:.7f}".format(i + 1, epoch + 1, loss.item()))
                    speed = (time.time() - time_now) / iter_count
                    left_time = speed * ((train_epochs - epoch) * train_steps - i)
                    print('\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))
                    iter_count = 0
                    time_now = time.time()
                loss.backward()
                model_optim.step()

            print("Epoch: {} cost time: {}".format(epoch + 1, time.time() - epoch_time))
            train_loss = np.average(train_loss)
            vali_loss = self.vali(vali_loader, criterion)
            test_loss = self.vali(test_loader, criterion)
            print("Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}".format(
                epoch + 1, train_steps, train_loss, vali_loss, test_loss))
            early_stopping(vali_loss, self.model, self.checkpoints_path)
            if early_stopping.early_stop:
                print("Early stopping")
                break
            adjust_learning_rate(model_optim, epoch + 1, self.learning_rate, lradj='type1')

        best_model_path = self.checkpoints_path + '/' + 'checkpoint.pth'
        self.model.load_state_dict(torch.load(best_model_path))

        return self.model

    def test(self, test_loader, test_name):
        self.model.eval()
        preds = []
        trues = []
        for i, (batch_enc, batch_enc_mark, batch_dec, batch_dec_mark, batch_output) in enumerate(test_loader):
            batch_prediction, batch_output = self._process_one_batch(batch_enc, batch_enc_mark,
                                                                     batch_dec, batch_dec_mark, batch_output)
            preds.append(batch_prediction.detach().cpu().numpy())
            trues.append(batch_output.detach().cpu().numpy())

        preds = np.array(preds)
        trues = np.array(trues)

        preds = preds.reshape(-1, self.out_len)
        preds = preds * (4.2 - 2.7) + 2.7
        trues = trues.reshape(-1, self.out_len)
        trues = trues * (4.2 - 2.7) + 2.7

        print('test shape:', preds.shape, trues.shape)

        mae, mse, rmse, mape, mspe = metric(preds, trues)
        print('mse:{}, mae:{}'.format(mse, mae))
        pd.DataFrame(np.array([[mae, mse, rmse, mape, mspe]])).to_csv(test_name + 'metrics.csv', header=False,
                                                                      index=False)
        pd.DataFrame(preds).to_csv(test_name + 'preds.csv', header=False, index=False)
        pd.DataFrame(trues).to_csv(test_name + 'trues.csv', header=False, index=False)

        return None

    def _process_one_batch(self, batch_enc, batch_enc_mark, batch_dec, batch_dec_mark, batch_output):
        batch_enc = batch_enc.float().to(self.device)
        batch_enc_mark = batch_enc_mark.float().to(self.device)
        batch_dec = batch_dec.float().to(self.device)
        batch_dec_mark = batch_dec_mark.float().to(self.device)
        batch_output = batch_output.float().to(self.device)
        if self.output_attention:
            batch_prediction = self.model(batch_enc, batch_enc_mark, batch_dec, batch_dec_mark)[0]
        else:
            batch_prediction = self.model(batch_enc, batch_enc_mark, batch_dec, batch_dec_mark)

        return batch_prediction, batch_output


path = '.'
data_generation = Dataset_generation(
    path_train_enc=path + 'train_discharge_enc.csv',
    path_train_dec=path + 'train_discharge_dec.csv',
    path_train_output=path + 'train_discharge_output.csv',
    path_vali_enc=path + 'validation_discharge_enc.csv',
    path_vali_dec=path + 'validation_discharge_dec.csv',
    path_vali_output=path + 'validation_discharge_output.csv',
    batch_size=16,
    seq_len=900,
    label_len=450,
    pred_len=900)
train_loader, vali_loader, test_loader = data_generation.return_dataloader(state='predict_voltage')
exp = Exp(enc_in=1, dec_in=1, c_out=1, seq_len=900, label_len=450, out_len=900,
          factor=5, d_model=512, n_heads=8, e_layers=[3, 2, 1], d_layers=2, d_ff=2048,
          dropout=0.05, attn='prob', activation='gelu',
          output_attention=False, distil=True, mix=True, patience=5)
print('>>>>>>>start training>>>>>>>>>>>>>>>>>>>>>>>>>>')
exp.train(train_loader, vali_loader, test_loader, train_epochs=240)
torch.cuda.empty_cache()
