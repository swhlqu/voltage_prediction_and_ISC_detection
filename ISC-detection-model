#!/usr/bin/env python
# -*- coding:utf-8 -*-
import torch
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.nn.functional as F
from math import sqrt
from torch import optim
import time
import warnings
from torch.utils.data import Dataset, DataLoader, TensorDataset
import math
from pathlib import Path
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
warnings.filterwarnings('ignore')


class Dataset_generation(Dataset):
    def __init__(self, path_train_x, path_train_y_cla, path_vali_x, path_vali_y_cla, path_test_x, path_test_y_cla,
                 batch_size, pred_len):
        self.path_train_x = path_train_x
        self.path_vali_x = path_vali_x
        self.path_test_x = path_test_x
        self.path_train_y_cla = path_train_y_cla
        self.path_vali_y_cla = path_vali_y_cla
        self.path_test_y_cla = path_test_y_cla
        self.batch_size = batch_size
        self.pred_len = pred_len
        self.x_train = None
        self.x_vali = None
        self.x_test = None
        self.y_train_cla = None
        self.y_vali_cla = None
        self.y_test_cla = None
        self.shuffle_index_train = None
        self.shuffle_index_vali = None
        self.shuffle_index_test = None

    def shuffle_data(self,x,y):
        np.random.seed(1)
        index = np.arange(x.shape[0])
        np.random.shuffle(index)
        x = x[index]
        y = y[index]
        return index, x, y

    def train(self):
        self.x_train = np.array(pd.read_csv(self.path_train_x, header=None)).reshape(-1, 2, self.pred_len).transpose(0,
                                                                                                                     2,
                                                                                                                     1)
       
        self.y_train_cla = np.array(pd.read_csv(self.path_train_y_cla, header=None)).reshape(-1, 1)
        self.shuffle_index_train, self.x_train, self.y_train_cla = self.shuffle_data(self.x_train,self.y_train_cla)
        train_set = TensorDataset(torch.from_numpy(self.x_train).to(torch.float32),
                                  torch.from_numpy(self.y_train_cla).to(torch.float32))
        print(self.x_train.shape, self.y_train_cla.shape)
        train_loader = DataLoader(train_set, batch_size=self.batch_size)
        return train_loader

    def vali(self):
        self.x_vali = np.array(pd.read_csv(self.path_vali_x, header=None)).reshape(-1, 2, self.pred_len).transpose(0, 2,
                                                                                                                   1)
        
        self.y_vali_cla = np.array(pd.read_csv(self.path_vali_y_cla, header=None)).reshape(-1, 1)
        self.shuffle_index_vali, self.x_vali, self.y_vali_cla = self.shuffle_data(self.x_vali, self.y_vali_cla)
        vali_set = TensorDataset(torch.from_numpy(self.x_vali).to(torch.float32),
                                 torch.from_numpy(self.y_vali_cla).to(torch.float32))
        print(self.x_vali.shape, self.y_vali_cla.shape)
        vali_loader = DataLoader(vali_set, batch_size=self.batch_size)
        return vali_loader

    def test(self):
        self.x_test = np.array(pd.read_csv(self.path_test_x, header=None)).reshape(-1, 2, self.pred_len).transpose(0, 2,
                                                                                                                   1)
       
        self.y_test_cla = np.array(pd.read_csv(self.path_test_y_cla, header=None)).reshape(-1, 1)
        self.shuffle_index_test, self.x_test, self.y_test_cla = self.shuffle_data(self.x_test, self.y_test_cla)
        test_set = TensorDataset(torch.from_numpy(self.x_test).to(torch.float32),
                                     torch.from_numpy(self.y_test_cla).to(torch.float32))
        print(self.x_test.shape, self.y_test_cla.shape)
        test_loader = DataLoader(test_set, batch_size=self.batch_size)
        return test_loader


def RSE(pred, true):
    return np.sqrt(np.sum((true - pred) ** 2)) / np.sqrt(np.sum((true - true.mean()) ** 2))


def CORR(pred, true):
    u = ((true - true.mean(0)) * (pred - pred.mean(0))).sum(0)
    d = np.sqrt(((true - true.mean(0)) ** 2 * (pred - pred.mean(0)) ** 2).sum(0))
    return (u / d).mean(-1)


def MAE(pred, true):
    return np.mean(np.abs(pred - true))


def MSE(pred, true):
    return np.mean((pred - true) ** 2)


def RMSE(pred, true):
    return np.sqrt(MSE(pred, true))


def MAPE(pred, true):
    return np.mean(np.abs((pred - true) / true))


def MSPE(pred, true):
    return np.mean(np.square((pred - true) / true))

def Accuracy(pred, true):
    return ((pred>0.5).float() == true).sum().item()/true.shape[0]


def metric(pred, true):
    mae = MAE(pred, true)
    mse = MSE(pred, true)
    rmse = RMSE(pred, true)
    mape = MAPE(pred, true)
    mspe = MSPE(pred, true)

    return mae, mse, rmse, mape, mspe


def adjust_learning_rate(optimizer, epoch, learning_rate, lradj):
    if lradj == 'type1':
        lr_adjust = {epoch: learning_rate * (0.8 ** ((epoch - 1) // 2))}
    elif lradj == 'type2':
        lr_adjust = {
            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,
            10: 5e-7, 15: 1e-7, 20: 5e-8
        }
    if epoch in lr_adjust.keys():
        lr = lr_adjust[epoch]
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        print('Updating learning rate to {}'.format(lr))


class EarlyStopping:
    def __init__(self, patience=7, verbose=False, delta=0):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf
        self.delta = delta

    def __call__(self, val_loss, model, path):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path)
        elif score < self.best_score + self.delta:
            self.counter += 1
            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model, path)
            self.counter = 0

    def save_checkpoint(self, val_loss, model, path):
        if self.verbose:
            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')
        torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')
        self.val_loss_min = val_loss


class TokenEmbedding(nn.Module):
    def __init__(self, c_in, d_model): 
        super(TokenEmbedding, self).__init__()
        padding = 1 if torch.__version__ >= '1.5.0' else 2
        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,
                                   kernel_size=3, padding=padding, padding_mode='circular')
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')

    def forward(self, x):
        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)
        return x


class PositionalEmbedding(nn.Module):
    def __init__(self, d_model, max_len=1000):
        super(PositionalEmbedding, self).__init__()
        pe = torch.zeros(max_len, d_model).float()  
        pe.require_grad = False

        position = torch.arange(0, max_len).float().unsqueeze(1) 
        div_term = (torch.arange(0, d_model, 2).float() * -(
                math.log(10000.0) / d_model)).exp()  

        pe[:, 0::2] = torch.sin(position * div_term)  
        pe[:, 1::2] = torch.cos(position * div_term)  

        pe = pe.unsqueeze(0) 
        self.register_buffer('pe', pe)  

    def forward(self, x):
        return self.pe[:, :x.size(1)] 


class DataEmbedding(nn.Module):
    def __init__(self, c_in, d_model, dropout=0.05):
        super(DataEmbedding, self).__init__()

        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model) 
        self.position_embedding = PositionalEmbedding(d_model=d_model)  
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x):
        x = self.value_embedding(x) + self.position_embedding(x)

        return self.dropout(x)


class FullAttention(nn.Module):
    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):
        super(FullAttention, self).__init__()
        self.scale = scale
        self.mask_flag = mask_flag  
        self.output_attention = output_attention
        self.dropout = nn.Dropout(attention_dropout)

    def forward(self, queries, keys, values, attn_mask):
        B, L, H, E = queries.shape
        _, S, _, D = values.shape
        scale = self.scale or 1. / sqrt(E)

        scores = torch.einsum("blhe,bshe->bhls", queries, keys)
        if self.mask_flag:
            if attn_mask is None:
                attn_mask = TriangularCausalMask(B, L, device=queries.device)

            scores.masked_fill_(attn_mask.mask, -np.inf)

        A = self.dropout(torch.softmax(scale * scores, dim=-1))
        V = torch.einsum("bhls,bshd->blhd", A, values)

        if self.output_attention:
            return (V.contiguous(), A)
        else:
            return (V.contiguous(), None)


class AttentionLayer(nn.Module):
    def __init__(self, attention, d_model, n_heads,
                 d_keys=None, d_values=None, mix=False):
        super(AttentionLayer, self).__init__()

        d_keys = d_keys or (d_model // n_heads)  
        d_values = d_values or (d_model // n_heads)  

        self.inner_attention = attention  
        self.query_projection = nn.Linear(d_model, d_keys * n_heads)  
        self.key_projection = nn.Linear(d_model, d_keys * n_heads)
        self.value_projection = nn.Linear(d_model, d_values * n_heads)
        self.out_projection = nn.Linear(d_values * n_heads, d_model)
        self.n_heads = n_heads
        self.mix = mix 

    def forward(self, queries, keys, values, attn_mask):
        B, L, _ = queries.shape
        _, S, _ = keys.shape
        H = self.n_heads  
        # B=batch_size,L=S=seq_len,H=heads
        queries = self.query_projection(queries).view(B, L, H, -1)  
        keys = self.key_projection(keys).view(B, S, H, -1)  
        values = self.value_projection(values).view(B, S, H, -1)  

        out, attn = self.inner_attention(
            queries,
            keys,
            values,
            attn_mask  
        )  
        if self.mix:  
            out = out.transpose(2, 1).contiguous()  
        out = out.view(B, L, -1) 

        return self.out_projection(out), attn  


class ConvLayer(nn.Module):
    def __init__(self, c_in):
        super(ConvLayer, self).__init__()
        padding = 1 if torch.__version__ >= '1.5.0' else 2
        self.downConv = nn.Conv1d(in_channels=c_in,
                                  out_channels=c_in,
                                  kernel_size=3,
                                  padding=padding,
                                  padding_mode='circular')
        self.norm = nn.BatchNorm1d(c_in)
        self.activation = nn.ELU()
        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)

    def forward(self, x):
        x = self.downConv(x.permute(0, 2, 1))
        x = self.norm(x)
        x = self.activation(x)
        x = x.transpose(1, 2)
        return x


class EncoderLayer(nn.Module):
    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation="relu"):
        super(EncoderLayer, self).__init__()
        d_ff = d_ff or 4 * d_model
        self.attention = attention  
        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)
        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = F.relu if activation == "relu" else F.gelu

    def forward(self, x, attn_mask=None):
        
        new_x, attn = self.attention(
            x, x, x,
            attn_mask=attn_mask
        ) 
        x = x + self.dropout(new_x) 

        y = x = self.norm1(x) 
        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))
        y = self.dropout(self.conv2(y).transpose(-1, 1))  

        return self.norm2(x + y), attn


class Encoder(nn.Module):
    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):
        super(Encoder, self).__init__()
        self.attn_layers = nn.ModuleList(attn_layers)  
        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None
        self.norm = norm_layer

    def forward(self, x, attn_mask=None):
        attns = []
        if self.conv_layers is not None:
            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers): 
                x, attn = attn_layer(x, attn_mask=attn_mask) 
                x = conv_layer(x)  
                attns.append(attn)
            x, attn = self.attn_layers[-1](x, attn_mask=attn_mask)
            attns.append(attn)
        else:
            for attn_layer in self.attn_layers:
                x, attn = attn_layer(x, attn_mask=attn_mask)
                attns.append(attn)

        if self.norm is not None:
            x = self.norm(x)
        return x, attns

class EncoderStack(nn.Module):
    def __init__(self, encoders, inp_lens):
        super(EncoderStack, self).__init__()
        self.encoders = nn.ModuleList(encoders)
        self.inp_lens = inp_lens

    def forward(self, x, attn_mask=None):
        x_stack = []
        attns = []
        for i_len, encoder in zip(self.inp_lens, self.encoders):  
            inp_len = x.shape[1] // (2 ** i_len)
            x_s, attn = encoder(x[:, -inp_len:, :])
            x_stack.append(x_s)
            attns.append(attn)
        x_stack = torch.cat(x_stack, -2)

        return x_stack, attns
class DecoderLayer(nn.Module):

    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,
                 dropout=0.1, activation="relu"):
        super(DecoderLayer, self).__init__()
        d_ff = d_ff or 4 * d_model
        self.self_attention = self_attention  
        self.cross_attention = cross_attention  
        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)
        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = F.relu if activation == "relu" else F.gelu 

    def forward(self, x, cross, x_mask=None, cross_mask=None):
        x = x + self.dropout(self.self_attention(
            x, x, x,
            attn_mask=x_mask
        )[0])
        x = self.norm1(x)

        x = x + self.dropout(self.cross_attention(
            x, cross, cross,
            attn_mask=cross_mask
        )[0])

        y = x = self.norm2(x)
        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))
        y = self.dropout(self.conv2(y).transpose(-1, 1))

        return self.norm3(x + y)


class Decoder(nn.Module):
    def __init__(self, layers, norm_layer=None):
        super(Decoder, self).__init__()
        self.layers = nn.ModuleList(layers)
        self.norm = norm_layer

    def forward(self, x, cross, x_mask=None, cross_mask=None):  
        for layer in self.layers:
            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)

        if self.norm is not None:
            x = self.norm(x)

        return x

class Trans(nn.Module):
    def __init__(self, enc_in, c_out, pred_len,
                 factor=5, d_model=64, n_heads=8, e_layers=3, d_ff=64,
                 dropout=0.0, attn='full', activation='gelu',
                 output_attention=False, distil=True):
        super(Trans, self).__init__()
        self.attn = attn  
        self.output_attention = output_attention
        self.enc_embedding = DataEmbedding(enc_in, d_model, dropout)

        Attn = FullAttention
        self.encoder = Encoder(
            [
                EncoderLayer(
                    AttentionLayer(Attn(False, factor, attention_dropout=dropout, output_attention=output_attention),
                                   d_model, n_heads, mix=False),
                    d_model,
                    d_ff,
                    dropout=dropout,
                    activation=activation
                ) for l in range(e_layers)
            ],
            [
                ConvLayer(
                    d_model
                ) for l in range(e_layers - 1)
            ] if distil else None,
            norm_layer=torch.nn.LayerNorm(d_model)
        )
        self.projection = nn.Linear(d_model, c_out, bias=True)
        self.flatten = nn.Flatten()
        self.projection2 = nn.Linear(pred_len, 1, bias=True)

    def forward(self, x, enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):
        enc_out = self.enc_embedding(x)
        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)
        dec_out = F.tanh(self.projection(enc_out))  
        dec_out = self.flatten(dec_out)
        out = F.sigmoid(self.projection2(dec_out)) 
        return out


class Exp():
    def __init__(self, enc_in, c_out, pred_len, factor, d_model, n_heads, e_layers, d_ff, dropout, attn,
                 activation, output_attention, distil, patience, checkpoints_path):
        super(Exp, self).__init__()
        self.enc_in = enc_in
        self.c_out = c_out
        self.pred_len = pred_len
        self.factor = factor
        self.d_model = d_model
        self.n_heads = n_heads
        self.e_layers = e_layers
        self.d_ff = d_ff
        self.dropout = dropout
        self.attn = attn
        self.activation = activation
        self.output_attention = output_attention
        self.distil = distil
        self.patience = patience
        self.learning_rate = 0.001
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = Trans(enc_in, c_out, pred_len, factor, d_model, n_heads, e_layers, d_ff, dropout, attn,
                              activation, output_attention, distil).to(self.device)
        self.checkpoints_path = checkpoints_path

    def _select_optimizer(self):
        model_optim = optim.Adam(self.model.parameters(), lr=self.learning_rate)
        return model_optim

    def _select_criterion(self):
        criterion = nn.BCELoss()
        return criterion

    def vali(self, vali_loader, criterion):
        self.model.eval()  
        total_loss = []
        vali_accuracy = []
        for i, (batch_x, batch_y) in enumerate(vali_loader):
            batch_prediction, batch_output = self._process_one_batch(batch_x, batch_y)
            loss = criterion(batch_prediction.detach().cpu(), batch_output.detach().cpu())
            accuracy = Accuracy(batch_prediction.detach().cpu(), batch_output.detach().cpu())
            total_loss.append(loss)
            vali_accuracy.append(accuracy)
        total_loss = np.average(total_loss)
        vali_accuracy = np.average(vali_accuracy)
        self.model.train()
        return total_loss, vali_accuracy

    def train(self, train_loader, vali_loader, test_loader, train_epochs):
        my_file = Path(self.checkpoints_path + '/' + 'checkpoint.pth')
        if my_file.exists():
            best_model_path = self.checkpoints_path + '/' + 'checkpoint.pth'
            self.model.load_state_dict(torch.load(best_model_path))
            print('have loader best model')
        time_now = time.time()
        train_steps = len(train_loader)
        early_stopping = EarlyStopping(self.patience, verbose=True)
        model_optim = self._select_optimizer()  # adam,learning_rate
        criterion = self._select_criterion()  # mse
        for epoch in range(train_epochs):
            iter_count = 0
            train_loss = []
            train_accuracy = []
            epoch_time = time.time()
            self.model.train()
            for i, (batch_x, batch_y) in enumerate(train_loader):
                iter_count += 1
                model_optim.zero_grad()
                batch_prediction, batch_output = self._process_one_batch(batch_x, batch_y)
                loss = criterion(batch_prediction, batch_output)
                accuracy = Accuracy(batch_prediction, batch_output)
                train_loss.append(loss.item())
                train_accuracy.append(accuracy)
                if (i + 1) % 100 == 0:
                    print("\titers: {0}, epoch: {1} | loss: {2:.7f} accuracy: {3:.5f}".format(i + 1, epoch + 1, loss.item(), accuracy))
                    speed = (time.time() - time_now) / iter_count
                    left_time = speed * ((train_epochs - epoch) * train_steps - i)
                    print('\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))
                    iter_count = 0
                    time_now = time.time()
                loss.backward()
                model_optim.step()

            print("Epoch: {} cost time: {}".format(epoch + 1, time.time() - epoch_time))
            train_loss = np.average(train_loss)
            train_accuracy = np.average(train_accuracy)
            vali_loss, vali_accuracy = self.vali(vali_loader, criterion)
            test_loss, test_accuracy = self.vali(test_loader, criterion)
            print("Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f} Train accyracy: {5:.5f} Vali accyracy: {5:.5f} Test accyracy: {5:.5f}".format(
                    epoch + 1, train_steps, train_loss, vali_loss, test_loss, train_accuracy, vali_accuracy, test_accuracy))
            early_stopping(vali_loss, self.model, self.checkpoints_path)
            if early_stopping.early_stop:
                print("Early stopping")
                break
            adjust_learning_rate(model_optim, epoch + 1, self.learning_rate, lradj='type1')

        best_model_path = self.checkpoints_path + '/' + 'checkpoint.pth'
        self.model.load_state_dict(torch.load(best_model_path))

        return self.model

    def test(self, test_loader, shuffle_index_test):
        self.model.eval()
        preds = []
        trues = []
        for i, (batch_x, batch_y) in enumerate(test_loader):
            batch_prediction, batch_output = self._process_one_batch(batch_x, batch_y)
            preds.append(batch_prediction.detach().cpu().numpy())
            trues.append(batch_output.detach().cpu().numpy())

        preds_last = preds.pop(-1)
        trues_last = trues.pop(-1)

        preds_last = preds_last.reshape(-1, 1)
        trues_last = trues_last.reshape(-1, 1)

        preds = np.array(preds)
        trues = np.array(trues)

        preds = preds.reshape(-1, 1)
        trues = trues.reshape(-1, 1)

        preds = np.concatenate((preds, preds_last), axis=0)
        trues = np.concatenate((trues, trues_last), axis=0)

        print('test shape:', preds.shape, trues.shape)

        mae, mse, rmse, mape, mspe = metric(preds, trues)
        print('mse:{}, mae:{}'.format(mse, mae))
        
        pd.concat([pd.DataFrame(np.array(shuffle_index_test).reshape(-1, 1)), pd.DataFrame(trues), pd.DataFrame(preds),
                   pd.DataFrame((preds>0.5).astype(int)), pd.DataFrame(np.abs(trues - (preds>0.5).astype(int)))],
                  axis=1, ignore_index=True).to_csv\
            (self.checkpoints_path + '/' + 'cla_results.csv', header=False, index=False)

        return None

    def predict(self, test_loader, shuffle_index_test):
        my_file = Path(self.checkpoints_path + '/' + 'checkpoint.pth')
        if my_file.exists():
            best_model_path = self.checkpoints_path + '/' + 'checkpoint.pth'
            self.model.load_state_dict(torch.load(best_model_path))
            print('have loader best model')
        self.model.eval()
        preds = []
        trues = []
        for i, (batch_x, batch_y) in enumerate(test_loader):
            batch_prediction, batch_output = self._process_one_batch(batch_x, batch_y)
            preds.append(batch_prediction.detach().cpu().numpy())
            trues.append(batch_output.detach().cpu().numpy())

        preds_last = preds.pop(-1)
        trues_last = trues.pop(-1)

        preds_last = preds_last.reshape(-1, 1)
        trues_last = trues_last.reshape(-1, 1)

        preds = np.array(preds)
        trues = np.array(trues)

        preds = preds.reshape(-1, 1)
        trues = trues.reshape(-1, 1)

        preds = np.concatenate((preds,preds_last),axis=0)
        trues = np.concatenate((trues,trues_last),axis=0)

        print('test shape:', preds.shape, trues.shape)

        mae, mse, rmse, mape, mspe = metric(preds, trues)
        print('mse:{}, mae:{}'.format(mse, mae))
        pd.DataFrame(np.array([[mae, mse, rmse, mape, mspe]])).to_csv('metrics.csv', header=False,
                                                                      index=False)
        pd.concat([pd.DataFrame(np.array(shuffle_index_test).reshape(-1, 1)), pd.DataFrame(trues), pd.DataFrame(preds),
                   pd.DataFrame((preds > 0.5).astype(int)), pd.DataFrame(np.abs(trues - (preds > 0.5).astype(int)))],
                  axis=1, ignore_index=True).to_csv \
            (self.checkpoints_path + '/' + 'cla_results.csv', header=False, index=False)

        return

    def _process_one_batch(self, batch_x, batch_y):
        batch_x = batch_x.float().to(self.device)
        batch_y = batch_y.float().to(self.device)
        if self.output_attention:
            batch_prediction = self.model(batch_x)[0]
        else:
            batch_prediction = self.model(batch_x)

        return batch_prediction, batch_y


dataset = Dataset_generation(
            path_train_x=    '.',
            path_train_y_cla='.',
            path_vali_x=     '.',
            path_vali_y_cla= '.',
            path_test_x=     '.',
            path_test_y_cla= '.',
            batch_size=128*8,
            pred_len=120)
train_loader = dataset.train()
vali_loader = dataset.vali()
test_loader = dataset.test()
shufflle_index_valid = dataset.shuffle_index_vali
shuffle_index_test = dataset.shuffle_index_test
exp = Exp(enc_in=2, c_out=1, pred_len=120, factor=5, d_model=64, n_heads=8, e_layers=3, d_ff=64*4, dropout=0.05,
                  attn='prob', activation='gelu', output_attention=False, distil=True, patience=10,
                  checkpoints_path=
                  ".")

print('>>>>>>>predicting<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')
exp.predict(vali_loader, shufflle_index_valid)
torch.cuda.empty_cache()
